{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Shared Bike systems\n",
    "\n",
    "## Téléchargement des données\n",
    "\n",
    "On télécharge les fichiers à partir des URLs fournies et les enregistre localement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Liste des années pour lesquelles nous voulons télécharger les données\n",
    "years = range(2014, 2024)\n",
    "\n",
    "# URL de base pour les fichiers de données\n",
    "base_url = \"https://s3.amazonaws.com/tripdata\"\n",
    "\n",
    "# Dossier où les fichiers seront enregistrés\n",
    "data_dir = \"citibike_data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Fonction pour télécharger un fichier\n",
    "def download_file(url, dest_path):\n",
    "    if not os.path.exists(dest_path):\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(dest_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            print(f\"Downloaded: {dest_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to download: {url}\")\n",
    "    else:\n",
    "        print(f\"File already exists: {dest_path}\")\n",
    "\n",
    "# Télécharger les fichiers pour chaque année\n",
    "for year in years:\n",
    "        # Construire les différentes variations de noms de fichiers possibles\n",
    "        possible_file_names = [\n",
    "            f\"{year}-citibike-tripdata.zip\",\n",
    "        ]\n",
    "\n",
    "        downloaded = False\n",
    "        for file_name in possible_file_names:\n",
    "            file_url = f\"{base_url}/{file_name}\"\n",
    "            \n",
    "            # Chemin de destination pour enregistrer le fichier\n",
    "            dest_path = os.path.join(data_dir, file_name)\n",
    "            \n",
    "            # Télécharger le fichier\n",
    "            download_file(file_url, dest_path)\n",
    "            if os.path.exists(dest_path):\n",
    "                downloaded = True\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction des fichiers ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import glob\n",
    "\n",
    "zip_dir = \"citibike_data\"\n",
    "csv_dir = \"citibike_data_csv\"\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "# Fonction pour extraire les fichiers ZIP et imprimer les fichiers CSV extraits\n",
    "def extract_zip_files(zip_dir, csv_dir):\n",
    "    zip_files = glob.glob(os.path.join(zip_dir, \"*.zip\"))\n",
    "    for zip_file in zip_files:\n",
    "        # Obtenir le nom du fichier sans extension\n",
    "        base_name = os.path.basename(zip_file).replace('.zip', '')\n",
    "        # Vérifier si le fichier CSV existe déjà dans le répertoire cible\n",
    "        if not any(glob.glob(os.path.join(csv_dir, f\"{base_name}*.csv\"))):\n",
    "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                zip_ref.extractall(csv_dir)\n",
    "            print(f\"Extracted: {zip_file}\")\n",
    "            # Lister les fichiers CSV extraits\n",
    "            extracted_files = glob.glob(os.path.join(csv_dir, f\"{base_name}*.csv\"))\n",
    "            for extracted_file in extracted_files:\n",
    "                print(f\"Fichier CSV extrait : {os.path.basename(extracted_file)}\")\n",
    "        else:\n",
    "            print(f\"File already extracted: {zip_file}\")\n",
    "\n",
    "# Extraire les fichiers ZIP et imprimer les fichiers CSV extraits\n",
    "extract_zip_files(zip_dir, csv_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'une session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Créer une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CitiBikeData\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.num.executors\", \"4\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde des données en un format parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "# Colonnes avant 2020\n",
    "columns_2014_2020 = [\n",
    "    \"tripduration\", \"starttime\", \"stoptime\", \"start station id\", \"start station name\",\n",
    "    \"start station latitude\", \"start station longitude\", \"end station id\", \"end station name\",\n",
    "    \"end station latitude\", \"end station longitude\", \"bikeid\", \"usertype\", \"birth year\", \"gender\"\n",
    "]\n",
    "\n",
    "# Colonnes après 2020\n",
    "column_mapping_2021 = {\n",
    "    \"ride_id\": \"trip_id\",\n",
    "    \"rideable_type\": \"bike_type\",\n",
    "    \"started_at\": \"starttime\",\n",
    "    \"ended_at\": \"stoptime\",\n",
    "    \"start_station_name\": \"start_station_name\",\n",
    "    \"start_station_id\": \"start_station_id\",\n",
    "    \"end_station_name\": \"end_station_name\",\n",
    "    \"end_station_id\": \"end_station_id\",\n",
    "    \"start_lat\": \"start_station_latitude\",\n",
    "    \"start_lng\": \"start_station_longitude\",\n",
    "    \"end_lat\": \"end_station_latitude\",\n",
    "    \"end_lng\": \"end_station_longitude\",\n",
    "    \"member_casual\": \"usertype\"\n",
    "}\n",
    "\n",
    "# Colonnes avec noms en majuscules et espaces\n",
    "column_mapping_uppercase = {\n",
    "    \"Trip Duration\": \"tripduration\",\n",
    "    \"Start Time\": \"starttime\",\n",
    "    \"Stop Time\": \"stoptime\",\n",
    "    \"Start Station ID\": \"start_station_id\",\n",
    "    \"Start Station Name\": \"start_station_name\",\n",
    "    \"Start Station Latitude\": \"start_station_latitude\",\n",
    "    \"Start Station Longitude\": \"start_station_longitude\",\n",
    "    \"End Station ID\": \"end_station_id\",\n",
    "    \"End Station Name\": \"end_station_name\",\n",
    "    \"End Station Latitude\": \"end_station_latitude\",\n",
    "    \"End Station Longitude\": \"end_station_longitude\",\n",
    "    \"Bike ID\": \"bikeid\",\n",
    "    \"User Type\": \"usertype\",\n",
    "    \"Birth Year\": \"birth_year\",\n",
    "    \"Gender\": \"gender\"\n",
    "}\n",
    "\n",
    "# Colonnes uniformisées\n",
    "uniform_columns = [\n",
    "    \"trip_id\", \"tripduration\", \"starttime\", \"stoptime\", \"start_station_id\", \"start_station_name\",\n",
    "    \"start_station_latitude\", \"start_station_longitude\", \"end_station_id\", \"end_station_name\",\n",
    "    \"end_station_latitude\", \"end_station_longitude\", \"bikeid\", \"bike_type\", \"usertype\", \n",
    "    \"birth_year\", \"gender\", \"year\", \"month\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_columns_exist(df, columns):\n",
    "    for col_name in columns:\n",
    "        if col_name not in df.columns:\n",
    "            df = df.withColumn(col_name, lit(None))\n",
    "    return df\n",
    "\n",
    "def process_csv_to_parquet_2014_2020(spark_df, year, month):\n",
    "    spark_df = spark_df.toDF(*columns_2014_2020).withColumnRenamed(\"start station id\", \"start_station_id\") \\\n",
    "        .withColumnRenamed(\"start station name\", \"start_station_name\") \\\n",
    "        .withColumnRenamed(\"start station latitude\", \"start_station_latitude\") \\\n",
    "        .withColumnRenamed(\"start station longitude\", \"start_station_longitude\") \\\n",
    "        .withColumnRenamed(\"end station id\", \"end_station_id\") \\\n",
    "        .withColumnRenamed(\"end station name\", \"end_station_name\") \\\n",
    "        .withColumnRenamed(\"end station latitude\", \"end_station_latitude\") \\\n",
    "        .withColumnRenamed(\"end station longitude\", \"end_station_longitude\") \\\n",
    "        .withColumnRenamed(\"birth year\", \"birth_year\")\n",
    "\n",
    "    spark_df = spark_df.withColumn(\"starttime\", col(\"starttime\").cast(\"timestamp\")) \\\n",
    "        .withColumn(\"stoptime\", col(\"stoptime\").cast(\"timestamp\"))\n",
    "    \n",
    "    spark_df = spark_df.withColumn(\"bike_type\", lit(\"classic\"))\n",
    "    spark_df = spark_df.withColumn(\"trip_id\", monotonically_increasing_id().cast(\"string\"))\n",
    "\n",
    "    spark_df = spark_df.withColumn(\"tripduration\", col(\"tripduration\").cast(\"integer\"))\n",
    "    spark_df = spark_df.withColumn(\"birth_year\", col(\"birth_year\").cast(\"integer\"))\n",
    "    spark_df = spark_df.withColumn(\"gender\", col(\"gender\").cast(\"integer\"))\n",
    "\n",
    "    spark_df = spark_df.withColumn(\"year\", lit(year)).withColumn(\"month\", lit(month))\n",
    "    \n",
    "    spark_df = ensure_columns_exist(spark_df, uniform_columns)\n",
    "    \n",
    "    spark_df = spark_df.select(*uniform_columns)\n",
    "    \n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_to_parquet_2021(spark_df, year, month):\n",
    "    for original, new in column_mapping_2021.items():\n",
    "        spark_df = spark_df.withColumnRenamed(original, new)\n",
    "    \n",
    "    spark_df = spark_df.withColumn(\"starttime\", col(\"starttime\").cast(\"timestamp\")) \\\n",
    "        .withColumn(\"stoptime\", col(\"stoptime\").cast(\"timestamp\"))\n",
    "    \n",
    "    spark_df = spark_df.withColumn(\"tripduration\", (col(\"stoptime\").cast(\"long\") - col(\"starttime\").cast(\"long\")).cast(\"int\"))\n",
    "    spark_df = spark_df.withColumn(\"bikeid\", concat_ws(\"_\", col(\"trip_id\"), col(\"bike_type\")))\n",
    "\n",
    "    spark_df = spark_df.withColumn(\"year\", lit(year)).withColumn(\"month\", lit(month))\n",
    "    \n",
    "    spark_df = spark_df.withColumn(\"birth_year\", lit(None).cast(\"integer\")) \\\n",
    "        .withColumn(\"gender\", lit(None).cast(\"integer\"))\n",
    "    \n",
    "    spark_df = ensure_columns_exist(spark_df, uniform_columns)\n",
    "    \n",
    "    spark_df = spark_df.select(*uniform_columns)\n",
    "    \n",
    "    return spark_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(file_path, year, month):\n",
    "    spark_df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "    \n",
    "    columns = spark_df.columns\n",
    "    if set(columns_2014_2020).issubset(columns):\n",
    "        return process_csv_to_parquet_2014_2020(spark_df, year, month)\n",
    "    elif set(column_mapping_2021.keys()).issubset(columns):\n",
    "        return process_csv_to_parquet_2021(spark_df, year, month)\n",
    "    elif set(column_mapping_uppercase.keys()).issubset(columns):\n",
    "        for original, new in column_mapping_uppercase.items():\n",
    "            spark_df = spark_df.withColumnRenamed(original, new)\n",
    "        return process_csv_to_parquet_2014_2020(spark_df, year, month)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown columns in file: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"citibike_data_csv\"\n",
    "\n",
    "for year in range(2014, 2024):\n",
    "    year_path = os.path.join(data_dir, f\"{year}-citibike-tripdata\")\n",
    "    if os.path.exists(year_path):\n",
    "        for month_folder in os.listdir(year_path):\n",
    "            month_path = os.path.join(year_path, month_folder)\n",
    "            try:\n",
    "                print(f\"Processing: {month_folder}\")\n",
    "                month = month_folder.split('_')[1]\n",
    "            except IndexError:\n",
    "                continue\n",
    "            for file in glob.glob(f\"{month_path}/*.csv\"):\n",
    "                processed_df = process_csv(file, year, month)\n",
    "                if processed_df is not None:\n",
    "                    # Sauvegarder le DataFrame directement dans les répertoires partitionnés\n",
    "                    parquet_path = f\"parquet_files/year={year}/month={month}\"\n",
    "                    processed_df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "                    print(f\"Saved {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"trip_id\", StringType(), True),\n",
    "    StructField(\"tripduration\", IntegerType(), True),  # Forcing tripduration to String\n",
    "    StructField(\"starttime\", TimestampType(), True),\n",
    "    StructField(\"stoptime\", TimestampType(), True),\n",
    "    StructField(\"start_station_id\", StringType(), True),\n",
    "    StructField(\"start_station_name\", StringType(), True),\n",
    "    StructField(\"start_station_latitude\", StringType(), True),\n",
    "    StructField(\"start_station_longitude\", StringType(), True),\n",
    "    StructField(\"end_station_id\", StringType(), True),\n",
    "    StructField(\"end_station_name\", StringType(), True),\n",
    "    StructField(\"end_station_latitude\", StringType(), True),\n",
    "    StructField(\"end_station_longitude\", StringType(), True),\n",
    "    StructField(\"bikeid\", StringType(), True),\n",
    "    StructField(\"bike_type\", StringType(), True),\n",
    "    StructField(\"usertype\", StringType(), True),\n",
    "    StructField(\"birth_year\", IntegerType(), True),\n",
    "    StructField(\"gender\", IntegerType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"month\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df = None\n",
    "\n",
    "data_dir = \"citibike_data_csv\"\n",
    "\n",
    "for year in range(2014, 2024):\n",
    "    year_path = os.path.join(data_dir, f\"{year}-citibike-tripdata\")\n",
    "    if os.path.exists(year_path):\n",
    "        for month_folder in os.listdir(year_path):\n",
    "            month_path = os.path.join(year_path, month_folder)\n",
    "            try:\n",
    "                print(f\"Processing: {month_folder}\")\n",
    "                month = month_folder.split('_')[1]\n",
    "            except IndexError:\n",
    "                continue\n",
    "            for file in glob.glob(f\"{month_path}/*.csv\"):\n",
    "                processed_df = process_csv(file, year, month)\n",
    "                if processed_df is not None:\n",
    "                    # Fusionner le DataFrame actuel avec le DataFrame global\n",
    "                    if global_df is None:\n",
    "                        global_df = processed_df\n",
    "                    else:\n",
    "                        global_df = global_df.unionByName(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df.storageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organiser les données en un schéma en étoile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2014, 2024)\n",
    "months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "\n",
    "df = None\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        parquet_file_path = f\"parquet_files/year={year}/month={month}\"\n",
    "        if os.path.exists(parquet_file_path):\n",
    "            try:\n",
    "                month_df = spark.read.schema(schema).parquet(parquet_file_path)\n",
    "                if df is None:\n",
    "                    df = month_df\n",
    "                else:\n",
    "                    df = df.unionByName(month_df)\n",
    "                print(f\"Loaded data for {year} {month}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read Parquet files from {parquet_file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.schema(schema).parquet(\"parquet_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the StorageLevel of the dataframe after reading the csv files?\n",
    "df.storageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the number of partitions of the dataframe?\n",
    "print(df.rdd.getNumPartitions())\n",
    "#Is it possible to tune this number at loading time?\n",
    "#Yes, it is possible to tune the number of partitions at loading time by using the `option(\"spark.sql.files.maxPartitionBytes\", \"128MB\")` configuration. This configuration sets the maximum number of bytes to pack into a single partition when reading files.\n",
    "#Why would we want to modify the number of partitions when creating the parquet files?\n",
    "#We may want to modify the number of partitions when creating the Parquet files to ensure that the data is evenly distributed across the partitions. This can help improve the performance of queries and operations that are performed on the data, especially when the data is large and needs to be processed in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "table de dimension regroupant des informations concernant les stations (nom, identifiant(s), latitude, longitude, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Créer la table des stations à partir des informations de départ et d'arrivée des trajets\n",
    "start_stations_df = df.select(\n",
    "    col(\"start_station_id\").alias(\"station_id\"),\n",
    "    col(\"start_station_name\").alias(\"station_name\"),\n",
    "    col(\"start_station_latitude\").alias(\"latitude\"),\n",
    "    col(\"start_station_longitude\").alias(\"longitude\")\n",
    ").distinct()\n",
    "\n",
    "end_stations_df = df.select(\n",
    "    col(\"end_station_id\").alias(\"station_id\"),\n",
    "    col(\"end_station_name\").alias(\"station_name\"),\n",
    "    col(\"end_station_latitude\").alias(\"latitude\"),\n",
    "    col(\"end_station_longitude\").alias(\"longitude\")\n",
    ").distinct()\n",
    "\n",
    "    # Union des deux DataFrames pour obtenir toutes les stations\n",
    "stations_df = start_stations_df.union(end_stations_df).distinct()\n",
    "\n",
    "    # Sauvegarder la table des stations en format Parquet\n",
    "stations_df.write.mode(\"overwrite\").parquet(\"stations_parquet\")\n",
    "print(\"Saved stations dimension table as Parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations = spark.read.parquet(\"stations_parquet\")\n",
    "\n",
    "df_stations.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " table d'événements regroupant des informations sur les voyages avec des références à la table de dimension (les identifiants des stations de départ/arrivée)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_event_df = df.select(\n",
    "    col(\"trip_id\"),\n",
    "    col(\"tripduration\"),\n",
    "    col(\"starttime\"),\n",
    "    col(\"stoptime\"),\n",
    "    col(\"start_station_id\").alias(\"start_station_id\"),\n",
    "    col(\"end_station_id\").alias(\"end_station_id\"),\n",
    "    col(\"bikeid\"),\n",
    "    col(\"bike_type\"),\n",
    "    col(\"usertype\"),\n",
    "    col(\"birth_year\"),\n",
    "    col(\"gender\"),\n",
    "    col(\"year\"),\n",
    "    col(\"month\")\n",
    ")\n",
    "\n",
    "    # Sauvegarder la table des trajets en format Parquet avec partitionnement\n",
    "trips_event_df.write.mode(\"overwrite\").partitionBy(\"year\", \"month\").parquet(\"trips_parquet\")\n",
    "print(\"Saved trips event table as Parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_event = spark.read.parquet(\"trips_parquet\")\n",
    "\n",
    "df_trips_event.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyser une année de données avant la COVID et une année après la COVID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premierement, nous allons charger les données des années 2019 et 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_year(year):\n",
    "    df = spark.read.schema(schema).parquet(f\"parquet_files/year={year}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019 = load_data_for_year(2019)\n",
    "df_2021 = load_data_for_year(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule la distance parcourue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sqrt, pow\n",
    "\n",
    "def add_trip_distance(df):\n",
    "    df = df.withColumn(\"trip_distance\", sqrt(\n",
    "        pow(col(\"end_station_latitude\") - col(\"start_station_latitude\"), 2) +\n",
    "        pow(col(\"end_station_longitude\") - col(\"start_station_longitude\"), 2)\n",
    "    ))\n",
    "    return df\n",
    "\n",
    "def convert_distance_to_km(df):\n",
    "    km_per_degree = 111  # Approximatif\n",
    "    df = df.withColumn(\"trip_distance_km\", col(\"trip_distance\") * lit(km_per_degree))\n",
    "    return df\n",
    "\n",
    "\n",
    "df_2019 = add_trip_distance(df_2019)\n",
    "df_2021 = add_trip_distance(df_2021)\n",
    "\n",
    "df_2019 = convert_distance_to_km(df_2019)\n",
    "df_2021 = convert_distance_to_km(df_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all from pyspark.sql.functions\n",
    "df_2019.select(\"gender\").distinct().show()\n",
    "df_2021.select(\"gender\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque jour de la semaine de l'année 2019, on affiche la mediane de la distance parcourue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_trip_distance_distribution(df, year):\n",
    "    df = df.withColumn(\"day_of_week\", dayofweek(col(\"starttime\")))\n",
    "    distance_distribution = df.groupBy(\"day_of_week\").agg(expr(\"percentile_approx(trip_distance_km, 0.5)\").alias(\"median_distance\")).collect()\n",
    "    day_of_week = [row[\"day_of_week\"] for row in distance_distribution]\n",
    "    median_distance = [row[\"median_distance\"] for row in distance_distribution]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(day_of_week, median_distance, color='blue', alpha=0.7)\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Median Trip Distance (km)')\n",
    "    plt.title(f'Median Trip Distance Distribution by Day of Week in {year}')\n",
    "    plt.show()\n",
    "\n",
    "plot_trip_distance_distribution(df_2019, 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et pour l'année 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trip_distance_distribution(df_2021, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compte le nombre de trajets pour chaque couple de stations de départ/arrivée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va d'abord affiché le nombre de couple de stations départ/arrivée distinctes pour l'année 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afficher les couples départ/arrivée distincts\n",
    "station_pairs_df = df_2019.select(\"start_station_name\", \"end_station_name\").distinct()\n",
    "print(\"Nombre de couples de stations distincts en 2019 : \", station_pairs_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que le nombre est extrêmement , on pourra donc pas toutes les affichées. On va se limité aux 20 couples de stations les plus fréquenté "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_trip_count_by_location(df, year):\n",
    "    # Grouper par noms des stations de départ et d'arrivée et compter le nombre de trajets\n",
    "    trip_count = df.groupBy(\"start_station_name\", \"end_station_name\").count().orderBy(\"count\", ascending=False).limit(20).collect()\n",
    "    \n",
    "    # Préparer les données pour la visualisation\n",
    "    start_end_stations = [f\"{row['start_station_name']} -> {row['end_station_name']}\" for row in trip_count]\n",
    "    counts = [row[\"count\"] for row in trip_count]\n",
    "\n",
    "    # Créer un graphique en barres horizontales\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(start_end_stations, counts, color='green', alpha=0.7)\n",
    "    plt.xlabel('Number of Trips')\n",
    "    plt.ylabel('Start Station -> End Station')\n",
    "    plt.title(f'Top 20 Trip Counts by Pickup/Dropoff Location in {year}')\n",
    "    plt.show()\n",
    "\n",
    "plot_trip_count_by_location(df_2019, 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'année 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afficher les couples départ/arrivée distincts\n",
    "station_pairs_df = df_2021.select(\"start_station_name\", \"end_station_name\").distinct()\n",
    "print(\"Nombre de couples de stations distincts en 2021 : \", station_pairs_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pareil, le nombre est élevé, on se limitra aussi aux 20 couples les plus fréquenté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trip_count_by_location(df_2021, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcule la répartition de la distance parcourue selon le sexe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'année 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_trip_distance_by_gender(df, year):\n",
    "    # Convertir la colonne 'gender' en entier si nécessaire\n",
    "    df = df.withColumn(\"gender\", col(\"gender\").cast(\"int\"))\n",
    "\n",
    "    # Filtrer pour exclure les trajets où le genre est non spécifié (0) ou NULL\n",
    "    filtered_df = df.filter((col(\"gender\") != 0) & (col(\"gender\").isNotNull()))\n",
    "    \n",
    "    # Calculer la distance médiane par genre\n",
    "    distance_by_gender = filtered_df.groupBy(\"gender\").agg(expr(\"percentile_approx(trip_distance_km, 0.5)\").alias(\"median_distance\")).collect()\n",
    "    \n",
    "    # Mapper les valeurs de genre aux étiquettes\n",
    "    gender_labels = {1: \"Male\", 2: \"Female\"}\n",
    "    genders = [gender_labels[row[\"gender\"]] for row in distance_by_gender]\n",
    "    median_distance = [row[\"median_distance\"] for row in distance_by_gender]\n",
    "\n",
    "    # Créer un graphique en barres\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(genders, median_distance, color='purple', alpha=0.7)\n",
    "    plt.xlabel('Gender')\n",
    "    plt.ylabel('Median Trip Distance (km)')\n",
    "    plt.title(f'Median Trip Distance Distribution by Gender in {year}')\n",
    "    plt.show()\n",
    "\n",
    "# Générer les graphiques\n",
    "plot_trip_distance_by_gender(df_2019, 2019)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'année 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trip_distance_by_gender(df_2021, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcule la répartition de la distance parcourue pour les tranches d'âge (15-24,25-44,45-54,55-64,65+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'année 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_age_column(df, year):\n",
    "    # Calculer l'âge en fonction de l'année des données\n",
    "    df = df.withColumn(\"age\", (lit(year) - col(\"birth_year\")).cast(\"integer\"))\n",
    "    return df\n",
    "\n",
    "# Ajouter la colonne \"age\" aux DataFrames 2019 et 2021\n",
    "df_2019 = add_age_column(df_2019, 2019)\n",
    "df_2021 = add_age_column(df_2021, 2021)\n",
    "\n",
    "def plot_trip_distance_by_age_range(df, year):\n",
    "    df = df.withColumn(\"age_range\", \n",
    "                       when(col(\"age\").between(15, 24), \"15-24\")\n",
    "                       .when(col(\"age\").between(25, 44), \"25-44\")\n",
    "                       .when(col(\"age\").between(45, 54), \"45-54\")\n",
    "                       .when(col(\"age\").between(55, 64), \"55-64\")\n",
    "                       .when(col(\"age\") >= 65, \"65+\")\n",
    "                       .otherwise(\"Unknown\"))\n",
    "\n",
    "    distance_by_age_range = df.groupBy(\"age_range\").agg(expr(\"percentile_approx(trip_distance_km, 0.5)\").alias(\"median_distance\")).collect()\n",
    "    \n",
    "    # Trier les âges\n",
    "    sorted_age_ranges = sorted(distance_by_age_range, key=lambda row: [\"15-24\", \"25-44\", \"45-54\", \"55-64\", \"65+\", \"Unknown\"].index(row[\"age_range\"]))\n",
    "    \n",
    "    age_ranges = [row[\"age_range\"] for row in sorted_age_ranges if row[\"age_range\"] != \"Unknown\"]\n",
    "    median_distance = [row[\"median_distance\"] for row in sorted_age_ranges if row[\"age_range\"] != \"Unknown\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(age_ranges, median_distance, color='orange', alpha=0.7)\n",
    "    plt.xlabel('Age Range')\n",
    "    plt.ylabel('Median Trip Distance (km)')\n",
    "    plt.title(f'Median Trip Distance Distribution by Age Range in {year}')\n",
    "    plt.show()\n",
    "\n",
    "# Générer les graphiques\n",
    "plot_trip_distance_by_age_range(df_2019, 2019)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'année 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trip_distance_by_age_range(df_2021, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcule la répartition de la distance parcourue pour différents types de vélos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'année 2019 (Les données avant 2021 n'ont pas de type de vélo, donc il y'aura que le type `classic` qu'on a mis par defaut qui sera affiché)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trip_distance_by_bike_type(df, year):\n",
    "    distance_by_bike_type = df.groupBy(\"bike_type\").agg(expr(\"percentile_approx(trip_distance_km, 0.5)\").alias(\"median_distance\")).collect()\n",
    "    bike_types = [row[\"bike_type\"] for row in distance_by_bike_type]\n",
    "    median_distance = [row[\"median_distance\"] for row in distance_by_bike_type]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(bike_types, median_distance, color='red', alpha=0.7)\n",
    "    plt.xlabel('Bike Type')\n",
    "    plt.ylabel('Median Trip Distance (km)')\n",
    "    plt.title(f'Median Trip Distance Distribution by Bike Type in {year}')\n",
    "    plt.show()\n",
    "\n",
    "plot_trip_distance_by_bike_type(df_2019, 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'année 2021 (il y'a aussi quelque mois qui n'ont pas de type de vélo, qui ont donc aussi la valeur par défaut `classic`, ce qui biaise le résultat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trip_distance_by_bike_type(df_2021, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Ajouter une colonne \"duration_category\" basée sur \"tripduration\"\n",
    "def add_duration_category(df):\n",
    "    df = df.withColumn(\"duration_category\", \n",
    "                       when(col(\"tripduration\") <= 300, \"Very Short\")\n",
    "                       .when((col(\"tripduration\") > 300) & (col(\"tripduration\") <= 900), \"Short\")\n",
    "                       .when((col(\"tripduration\") > 900) & (col(\"tripduration\") <= 1800), \"Medium\")\n",
    "                       .when((col(\"tripduration\") > 1800) & (col(\"tripduration\") <= 3600), \"Long\")\n",
    "                       .when(col(\"tripduration\") > 3600, \"Very Long\"))\n",
    "    return df\n",
    "\n",
    "# Appliquer la fonction aux DataFrames 2019 et 2021\n",
    "df_2019 = add_duration_category(df_2019)\n",
    "df_2021 = add_duration_category(df_2021)\n",
    "\n",
    "def plot_trip_distance_by_duration_category(df, year):\n",
    "    distance_by_duration = df.groupBy(\"duration_category\").agg(expr(\"percentile_approx(trip_distance_km, 0.5)\").alias(\"median_distance\")).collect()\n",
    "    \n",
    "    duration_categories = [row[\"duration_category\"] for row in distance_by_duration if row[\"duration_category\"] is not None]\n",
    "    median_distance = [row[\"median_distance\"] for row in distance_by_duration if row[\"duration_category\"] is not None]\n",
    "    \n",
    "    # Trier les catégories de durée\n",
    "    sorted_categories = [\"Very Short\", \"Short\", \"Medium\", \"Long\", \"Very Long\"]\n",
    "    duration_categories, median_distance = zip(*sorted(zip(duration_categories, median_distance), key=lambda x: sorted_categories.index(x[0])))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(duration_categories, median_distance, color='teal', alpha=0.7)\n",
    "    plt.xlabel('Trip Duration Category')\n",
    "    plt.ylabel('Median Trip Distance (km)')\n",
    "    plt.title(f'Median Trip Distance Distribution by Trip Duration in {year}')\n",
    "    plt.show()\n",
    "\n",
    "# Générer les graphiques\n",
    "plot_trip_distance_by_duration_category(df_2019, 2019)\n",
    "plot_trip_distance_by_duration_category(df_2021, 2021)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluer les saisonnalités et examiner les séries temporelles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on va calculer et tracer des série temporelle indexée par jour de la semaine et heure de la journée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commance d'abord par ajouter les colonnes `day_of_week` et `hour_of_day`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Ajouter les colonnes \"day_of_week\" et \"hour_of_day\"\n",
    "def add_time_columns(df):\n",
    "    df = df.withColumn(\"day_of_week\", dayofweek(col(\"starttime\")))\n",
    "    df = df.withColumn(\"hour_of_day\", hour(col(\"starttime\")))\n",
    "    return df\n",
    "\n",
    "# Appliquer la fonction aux DataFrames 2019 et 2021\n",
    "df_2019 = add_time_columns(df_2019)\n",
    "df_2021 = add_time_columns(df_2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on calcule les métriques et les trier par jour de la semaine et heure de la journée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(df):\n",
    "    metrics = df.groupBy(\"day_of_week\", \"hour_of_day\").agg(\n",
    "        count(\"trip_id\").alias(\"pickup_dock_count\"),\n",
    "        avg(\"trip_distance_km\").alias(\"avg_distance\"),\n",
    "        avg(\"tripduration\").alias(\"avg_duration\")\n",
    "    )\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_2019 = compute_metrics(df_2019).orderBy(\"day_of_week\", \"hour_of_day\").collect()\n",
    "metrics_2021 = compute_metrics(df_2021).orderBy(\"day_of_week\", \"hour_of_day\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On convertie ces métriques au format `Pandas` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pandas(metrics):\n",
    "    data = [(row[\"day_of_week\"], row[\"hour_of_day\"], row[\"pickup_dock_count\"], row[\"avg_distance\"], row[\"avg_duration\"]) for row in metrics]\n",
    "    df = pd.DataFrame(data, columns=[\"day_of_week\", \"hour_of_day\", \"pickup_dock_count\", \"avg_distance\", \"avg_duration\"])\n",
    "    return df\n",
    "\n",
    "metrics_2019_pd = convert_to_pandas(metrics_2019)\n",
    "metrics_2021_pd = convert_to_pandas(metrics_2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction qui sera utilisé pour tracer une série temporelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(df, metric, title, ylabel, year):\n",
    "    pivot_df = df.pivot(index='hour_of_day', columns='day_of_week', values=metric)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(pivot_df, cmap=\"YlGnBu\", annot=True, fmt=\".2f\", cbar_kws={'label': ylabel})\n",
    "    plt.title(f'{title} in {year}')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Hour of Day')\n",
    "    plt.xticks(ticks=[0.5,1.5,2.5,3.5,4.5,5.5,6.5], labels=[\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace le nombre de départ/arrivé pour les années 2019 et 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series(metrics_2019_pd, 'pickup_dock_count', 'Number of Pickups/Docks', 'Number of Pickups/Docks', 2019)\n",
    "plot_time_series(metrics_2021_pd, 'pickup_dock_count', 'Number of Pickups/Docks', 'Number of Pickups/Docks', 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace la distance moyenne pour les années 2019 et 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series(metrics_2019_pd, 'avg_distance', 'Average Trip Distance (km)', 'Average Trip Distance (km)', 2019)\n",
    "plot_time_series(metrics_2021_pd, 'avg_distance', 'Average Trip Distance (km)', 'Average Trip Distance (km)', 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace la durée moyenne des trajets pour les années 2019 et 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series(metrics_2019_pd, 'avg_duration', 'Average Trip Duration (sec)', 'Average Trip Duration (sec)', 2019)\n",
    "plot_time_series(metrics_2021_pd, 'avg_duration', 'Average Trip Duration (sec)', 'Average Trip Duration (sec)', 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les plans d'exécution\n",
    "metrics_2019 = compute_metrics(df_2019)\n",
    "metrics_2019.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorer les problèmes liés à l'information spatiale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construire une carte thermique indexée par couple de stations où la couleur est une fonction du nombre de trajets d'une station à une autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Calcul du nombre de trajets entre chaque couple de stations directement dans Spark\n",
    "trip_counts_spark = df_2019.groupBy('start_station_name', 'end_station_name').count().withColumnRenamed('count', 'trip_count')\n",
    "\n",
    "# Filtrer les données pour les stations les plus fréquentées (facultatif)\n",
    "top_stations = [row['start_station_name'] for row in trip_counts_spark.groupBy('start_station_name').sum('trip_count').orderBy('sum(trip_count)', ascending=False).limit(20).collect()]\n",
    "\n",
    "trip_counts_filtered = trip_counts_spark.filter(trip_counts_spark.start_station_name.isin(top_stations) & trip_counts_spark.end_station_name.isin(top_stations))\n",
    "\n",
    "# Convertir les données réduites en Pandas\n",
    "trip_counts_pd = trip_counts_filtered.toPandas()\n",
    "\n",
    "fig = px.density_heatmap(trip_counts_pd, x='start_station_name', y='end_station_name', z='trip_count',\n",
    "                         labels={'start_station_name': 'Start Station', 'end_station_name': 'End Station', 'trip_count': 'Number of Trips'},\n",
    "                         title='Heatmap of Trips between Top Stations')\n",
    "\n",
    "# Ajuster la taille de la figure et l'orientation des étiquettes\n",
    "fig.update_layout(width=1000, height=1000, margin=dict(l=200, r=200, b=200, t=100))\n",
    "fig.update_xaxes(tickangle=45, tickfont=dict(size=10))\n",
    "fig.update_yaxes(tickfont=dict(size=10))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construire une carte thermique interactive avec un curseur permettant à l'utilisateur de sélectionner une heure de la journée et où la couleur est une fonction du nombre de trajets d'une station à une autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019 = df_2019.withColumn('hour_of_day', hour(df_2019['starttime']))\n",
    "\n",
    "# Calculer le nombre de trajets entre chaque couple de stations pour chaque heure de la journée\n",
    "trip_counts_hourly = df_2019.groupBy('start_station_name', 'end_station_name', 'hour_of_day').count().withColumnRenamed('count', 'trip_count')\n",
    "\n",
    "# Filtrer les données pour les stations les plus fréquentées (facultatif)\n",
    "top_stations = [row['start_station_name'] for row in trip_counts_hourly.groupBy('start_station_name').sum('trip_count').orderBy('sum(trip_count)', ascending=False).limit(20).collect()]\n",
    "\n",
    "trip_counts_filtered_hourly = trip_counts_hourly.filter(trip_counts_hourly.start_station_name.isin(top_stations) & trip_counts_hourly.end_station_name.isin(top_stations))\n",
    "\n",
    "# Convertir les données en Pandas\n",
    "trip_counts_hourly_pd = trip_counts_filtered_hourly.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Créer une liste des figures pour chaque heure\n",
    "figures = []\n",
    "\n",
    "for hour in range(24):\n",
    "    hourly_data = trip_counts_hourly_pd[trip_counts_hourly_pd['hour_of_day'] == hour]\n",
    "    fig = px.density_heatmap(hourly_data, x='start_station_name', y='end_station_name', z='trip_count',\n",
    "                             labels={'start_station_name': 'Start Station', 'end_station_name': 'End Station', 'trip_count': 'Number of Trips'},\n",
    "                             title=f'Heatmap of Trips between Top Stations at Hour {hour}')\n",
    "    \n",
    "    # Ajuster la taille de la figure et l'orientation des étiquettes\n",
    "    fig.update_layout(width=1000, height=1000, margin=dict(l=200, r=200, b=200, t=100))\n",
    "    fig.update_xaxes(tickangle=45, tickfont=dict(size=10))\n",
    "    fig.update_yaxes(tickfont=dict(size=10))\n",
    "    \n",
    "    figures.append(fig)\n",
    "\n",
    "# Créer une figure avec des sliders\n",
    "fig = make_subplots(rows=1, cols=1, shared_xaxes=True, shared_yaxes=True, subplot_titles=['Heatmap of Trips between Top Stations'])\n",
    "\n",
    "# Ajouter des traces pour chaque heure\n",
    "for hour, hourly_fig in enumerate(figures):\n",
    "    for trace in hourly_fig.data:\n",
    "        fig.add_trace(trace)\n",
    "\n",
    "# Ajouter un slider\n",
    "steps = []\n",
    "for i in range(24):\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [False] * len(fig.data)}],  # Tout rendre invisible\n",
    "        label=f'Hour {i}'\n",
    "    )\n",
    "    for j in range(i * len(hourly_fig.data), (i + 1) * len(hourly_fig.data)):\n",
    "        step[\"args\"][0][\"visible\"][j] = True  # Rendre visible la trace correspondante\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    active=0,\n",
    "    pad={\"t\": 50},\n",
    "    steps=steps\n",
    ")]\n",
    "\n",
    "fig.update_layout(\n",
    "    sliders=sliders\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Superviser l'exécution des tâches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utilisation de la méthode explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2021.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Les difference entre le plan logique analysé et le plan logique optimisé :\n",
    "- Optimisation des Projections : Le Plan Logique Optimisé combine plusieurs projections en une seule projection lorsque c'est possible, réduisant ainsi la surcharge des projections intermédiaires.\n",
    "\n",
    "- Simplification des Constantes : Les constantes et les expressions déterministes sont calculées durant l'optimisation pour simplifier le plan.\n",
    "\n",
    "- Réduction des Colonnes : Seules les colonnes nécessaires sont conservées dans les projections, ce qui peut réduire la quantité de données traitées. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment un SGBDR procéderait avec une telle requête\n",
    "\n",
    "- Analyse Syntaxique : La requête SQL est analysée en un arbre syntaxique abstrait (AST) ou une structure similaire.\n",
    "- Analyse Sémantique : La requête analysée subit une analyse sémantique où les références de colonnes sont résolues et les types sont vérifiés.\n",
    "- Plan Logique : Un plan logique est généré, similaire au Plan Logique Analysé de Spark. Il décrit les opérations nécessaires pour exécuter la requête.\n",
    "- Optimisation : Le plan logique est optimisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Différences entre le Plan Physique et le Plan Logique Optimisé\n",
    "\n",
    "- Plan Logique Optimisé :  \n",
    "Le Plan Logique Optimisé représente une version simplifiée et rationalisée des opérations nécessaires pour exécuter la requête. Il supprime les opérations redondantes et combine les transformations lorsque c'est possible.\n",
    "\n",
    "- Plan Physique :  \n",
    "Le Plan Physique détaille la manière exacte dont chaque opération sera exécutée sur le cluster Spark. Il inclut des informations sur l'exécution réelle des tâches, comme l'utilisation de mémoire, les partitions, et les stratégies de traitement spécifiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mots-clés Non Attendus dans un SGBDR et Leur Signification\n",
    "\n",
    "- FileScan parquet :  \n",
    "Signification : Lecture de fichiers Parquet, un format de stockage en colonnes optimisé pour les requêtes analytiques.\n",
    "RDBMS : Les SGBDR traditionnels utilisent des fichiers de table internes ou des tablespaces plutôt que des fichiers de format spécifique comme Parquet.\n",
    "\n",
    "- Batched :  \n",
    "Signification : Traitement des données en lots pour améliorer l'efficacité.\n",
    "RDBMS : Bien que les SGBDR puissent utiliser des opérations par lots, ce terme spécifique est plus courant dans le traitement distribué comme Spark.\n",
    "\n",
    "- Union :  \n",
    "Signification : Combinaison de plusieurs ensembles de données.\n",
    "RDBMS : Bien que les SGBDR aient des opérations UNION, dans le contexte Spark, cela peut impliquer des manipulations de partitions supplémentaires pour gérer les données distribuées.\n",
    "\n",
    "- Project :  \n",
    "Signification : Sélection et transformation des colonnes.\n",
    "RDBMS : Bien que les projections existent dans les SGBDR, le terme Project en tant qu'opération distincte de l'optimisation logique est plus spécifique à Spark et aux frameworks de traitement de données distribués."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nombre de Stages Nécessaires pour Compléter le Job Spark\n",
    "\n",
    "Dans notre cas, le nombre de stages nécessaires est : 204"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rôles de `HashAggregate` et `Exchange hashpartitioning``\n",
    "\n",
    "- HashAggregate : \n",
    "* * HashAggregate est une étape d'agrégation dans laquelle les données sont agrégées en utilisant une fonction de hachage. Cela signifie que les valeurs des colonnes de regroupement sont transformées en codes de hachage, qui sont ensuite utilisés pour effectuer des agrégations.\n",
    "* * Typiquement utilisé pour des opérations comme groupBy, sum, avg, etc.\n",
    "\n",
    "- Exchange hashpartitioning : \n",
    "* * Exchange avec hashpartitioning indique un shuffle dans Spark où les données sont redistribuées à travers les partitions en utilisant une fonction de hachage sur une clé spécifique.\n",
    "* * Par exemple, lors d'un join ou d'un groupBy, les données doivent être partitionnées de manière à ce que les mêmes clés soient envoyées aux mêmes partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nombre d'operations `shuffle` utilisé par le plan physique\n",
    "\n",
    "Les opérations de `shuffle` sont indiquées par des `Exchange` dans le plan physique. Cependant, dans notre plan physique plus haut, il n'y a aucune mention explicite d'opérations Exchange. Cela signifie que le plan physique ne montre pas de redistributions de données explicites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Les Tâches dans le Contexte des Stages dans Spark\n",
    "\n",
    "Une tâche est une unité de travail qui exécute une fraction d'un stage. Chaque tâche traite une partition spécifique des données et est exécutée sur un nœud de travail dans le cluster. Le nombre de tâches par stage dépend du nombre de partitions des données d'entrée pour ce stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyse des Tâches dans nos Stages\n",
    "\n",
    "Nos stages sont constitué d'un nombre de tâches qui diffère en fonction des stages, entre 1, 2, 4, 5, 8, 55 et 60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
